xian-summer-2018

scp words.txt emr-master.xian-summer-2018.training:~/jfdai/
scp config-1.3.2.jar emr-master.xian-summer-2018.training:~/jfdai/
scp tw-pipeline/target/scala-2.11/tw-pipeline_2.11-0.1.0-SNAPSHOT.jar emr-master.xian-summer-2018.training:~/jfdai/

scp sample-data/sample_citybikes.csv emr-master.xian-summer-2018.training:~/jfdai/




spark-submit --conf spark.eventLog.enabled=true --jars config-1.3.2.jar --class   com.thoughtworks.ca.de.batch.wordcount.WordCount  --master local tw-pipeline/target/scala-2.11/tw-pipeline_2.11-0.1.0-SNAPSHOT.jar ~/Downloads/words.txt /tmp/words

spark-submit --conf spark.eventLog.enabled=true --jars config-1.3.2.jar --class   com.thoughtworks.ca.de.batch.wordcount.WordCount  --master local tw-pipeline_2.11-0.1.0-SNAPSHOT.jar words.txt hdfs://ip-10-0-3-215.ap-southeast-1.compute.internal:8020/user/hadoop/jfdai/results


spark-submit --conf spark.eventLog.enabled=true --jars config-1.3.2.jar --class   com.thoughtworks.ca.de.batch.ingest_to_data_lake.DailyDriver  --master local tw-pipeline/target/scala-2.11/tw-pipeline_2.11-0.1.0-SNAPSHOT.jar sample-data/sample_citybikes.csv /tmp/sample_citybikes


spark-submit --conf spark.eventLog.enabled=true --jars config-1.3.2.jar --class   com.thoughtworks.ca.de.batch.ingest_to_data_lake.DailyDriver  --master local tw-pipeline_2.11-0.1.0-SNAPSHOT.jar jfdai/raw/sample_citybikes.csv jfdai/raw/citybikes

spark-submit --conf spark.eventLog.enabled=true --jars config-1.3.2.jar --class   com.thoughtworks.ca.de.batch.citibike.CitibikeTransformer  --master local tw-pipeline_2.11-0.1.0-SNAPSHOT.jar jfdai/raw/citybikes jfdai/canonical/citybikes_distance

aws emr add-steps --cluster-id j-XXXXXXXX --steps Type=CUSTOM_JAR,Name=CustomJAR,ActionOnFailure=CONTINUE,Jar=s3://mybucket/mytest.jar,Args=arg1,arg2,arg3

Type=CUSTOM_JAR,Name=CustomJAR,ActionOnFailure=CONTINUE,Jar=s3://mybucket/mytest.jar,MainClass=mymainclass,Args=arg1,arg2,arg3


#Stop all running containers
docker stop $(docker ps -aq)

#Remove all containers
docker rm -f $(docker ps -aq)

docker exec -it streamingdatapipeline_hadoop_1 /bin/sh

/usr/local/hadoop/bin/hadoop fs -ls /free2wheelers/rawData/stationInformation/data

/usr/local/hadoop/bin/hadoop fs -ls /free2wheelers/stationMart/data

/usr/local/hadoop/bin/hadoop fs -cat 

/usr/local/hadoop/bin/hadoop fs -tail 

docker inspect --format='{{.Id}} {{.Parent}}' $(docker images --filter since=<image_id> -q)
docker inspect --format=’{{.Id}} {{.Parent}}’ $(docker images --filter since=631ab7abb432 -q)

for i in $(docker images -q)
do
    docker history $i | grep -q 631ab7abb432 && echo $i
done | sort -u


docker run -it -p 4040:4040 -p 8080:8080 -p 8081:8081 -h spark --name=spark docker-spark

